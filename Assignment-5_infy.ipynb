{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\sidhe\\Downloads\\cleaned_combined_dataset.xlsx\"\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Specify the columns for processing\n",
    "job_description_col = 'job_description'\n",
    "transcript_col = 'transcript'\n",
    "resume_col = 'resume'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing The Bert Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67f35a8967b4955854a5f1dc888730a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sidhe\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd75c2086d44c71a442197f3b2cfb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d15ba8fcd274f0190f9d4524e75e0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6a88fcdfd441cda79f9a91ffe0b6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2decbc4ea0584fe988bdb68d240d3260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the columns for processing\n",
    "job_description_col = 'job_description'\n",
    "transcript_col = 'transcript'\n",
    "resume_col = 'resume'\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to get BERT embeddings and Process text columns with BERT embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # Use the [CLS] token embedding as a summary of the text\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0).numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "# Process text columns with BERT embeddings\n",
    "def process_with_bert(column_name):\n",
    "    embeddings = []\n",
    "    for text in data[column_name].fillna(''):\n",
    "        embedding = get_bert_embeddings(text)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getting the enbeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for job descriptions...\n",
      "Generating BERT embeddings for transcripts...\n",
      "Generating BERT embeddings for resumes...\n",
      "Preparing for Word2Vec embeddings...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating BERT embeddings for job descriptions...\")\n",
    "data[f'{job_description_col}_bert'] = process_with_bert(job_description_col)\n",
    "\n",
    "print(\"Generating BERT embeddings for transcripts...\")\n",
    "data[f'{transcript_col}_bert'] = process_with_bert(transcript_col)\n",
    "\n",
    "print(\"Generating BERT embeddings for resumes...\")\n",
    "data[f'{resume_col}_bert'] = process_with_bert(resume_col)\n",
    "\n",
    "# Prepare for Word2Vec embeddings\n",
    "print(\"Preparing for Word2Vec embeddings...\")\n",
    "text_data = data[[job_description_col, transcript_col, resume_col]].fillna('').values.flatten()\n",
    "tokenized_data = [text.split() for text in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Word2Vec model and function to get Word2Vec embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get Word2Vec embeddings\n",
    "def get_word2vec_embeddings(text):\n",
    "    words = text.split()\n",
    "    embeddings = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process text columns with Word2Vec embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def process_with_word2vec(column_name):\n",
    "    embeddings = []\n",
    "    for text in data[column_name].fillna(''):\n",
    "        embedding = get_word2vec_embeddings(text)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Word2Vec embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Word2Vec embeddings for job descriptions...\n",
      "Generating Word2Vec embeddings for transcripts...\n",
      "Generating Word2Vec embeddings for resumes...\n",
      "Processed data saved to C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Word2Vec embeddings for job descriptions...\")\n",
    "data[f'{job_description_col}_word2vec'] = process_with_word2vec(job_description_col)\n",
    "\n",
    "print(\"Generating Word2Vec embeddings for transcripts...\")\n",
    "data[f'{transcript_col}_word2vec'] = process_with_word2vec(transcript_col)\n",
    "\n",
    "print(\"Generating Word2Vec embeddings for resumes...\")\n",
    "data[f'{resume_col}_word2vec'] = process_with_word2vec(resume_col)\n",
    "\n",
    "# Save processed data\n",
    "output_file = r'C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings.xlsx'\n",
    "data.to_excel(output_file, index=False)\n",
    "print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Glove Model for embeddings (Based on research on internet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model...\n",
      "Embedding column: job_description\n",
      "Embedding column: transcript\n",
      "Embedding column: resume\n",
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "               source_file source_sheet  \\\n",
      "0  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "1  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "2  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "3  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "4  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "\n",
      "                           job_description_embedding  \\\n",
      "0  [-0.06838082352941176, 0.16142741176470587, 0....   \n",
      "1  [-0.07749826666666665, 0.18295106666666663, 0....   \n",
      "2  [-0.07265462499999999, 0.17151662499999998, 0....   \n",
      "3  [-0.07265462499999999, 0.17151662499999998, 0....   \n",
      "4  [-0.06838082352941176, 0.16142741176470587, 0....   \n",
      "\n",
      "                                transcript_embedding  \\\n",
      "0  [-0.07743272709677428, 0.14780251290322574, 0....   \n",
      "1  [-0.09439831557496364, 0.12709692096069872, 0....   \n",
      "2  [-0.09132300704500977, 0.15501444794520533, 0....   \n",
      "3  [-0.09758575254010692, 0.16322203890374315, 0....   \n",
      "4  [-0.08310437412199626, 0.1506663049907578, 0.1...   \n",
      "\n",
      "                                    resume_embedding  \n",
      "0  [-0.09252163974358976, 0.11051320512820514, 0....  \n",
      "1  [-0.13412059818731115, 0.11648161117824783, 0....  \n",
      "2  [-0.12101620384615384, 0.16451516449704148, 0....  \n",
      "3  [-0.1170698708206688, 0.14475598480243168, 0.0...  \n",
      "4  [-0.11942216625916875, 0.14034793887530575, 0....  \n",
      "File saved to C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings_glove_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load GloVe embeddings (assuming you have the pre-trained GloVe file, e.g., 'glove.6B.100d.txt')\n",
    "def load_glove_model(glove_file):\n",
    "    print(\"Loading GloVe model...\")\n",
    "    glove_model = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=float)\n",
    "            glove_model[word] = vector\n",
    "    return glove_model\n",
    "\n",
    "# Function to get GloVe vector for a word (returns zero vector if word is not in the GloVe model)\n",
    "def get_glove_vector(word, glove_model, vector_size=100):\n",
    "    return glove_model.get(word, np.zeros(vector_size))\n",
    "\n",
    "# Load your Excel data\n",
    "csv_file = r\"B:\\OneDrive - Amity University\\Desktop\\Tower Research\\cleaned_combined_dataset.xlsx\"\n",
    "df = pd.read_excel(csv_file) \n",
    "\n",
    "# Load GloVe embeddings (replace with path to your GloVe file)\n",
    "glove_model = load_glove_model(\"B:\\OneDrive - Amity University\\Desktop\\glove.6B\\glove.6B.100d.txt\")\n",
    "\n",
    "# Select columns to apply GloVe embeddings\n",
    "columns_to_embed = ['job_description', 'transcript', 'resume']  # Replace with the columns you want to embed\n",
    "\n",
    "# Embed the columns\n",
    "for column in columns_to_embed:\n",
    "    print(f\"Embedding column: {column}\")\n",
    "    df[column + '_embedding'] = df[column].apply(lambda x: np.mean([get_glove_vector(word, glove_model) for word in str(x).split()], axis=0))\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Save the dataframe with the embeddings to an Excel file\n",
    "output_file = r'C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings_glove_2.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, index=False, sheet_name='Embeddings')\n",
    "\n",
    "print(f\"File saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pre proccesing the csv of embeddings for futther work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "               source_file source_sheet  \\\n",
      "0  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "1  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "2  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "3  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "4  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "\n",
      "                                job_description_bert  \\\n",
      "0  [-0.184033975, -0.102659881, -0.133955404, -0....   \n",
      "1  [-0.103227332, -0.115133069, -0.304571718, -0....   \n",
      "2  [-0.240335613, -0.219893694, -0.0979446471, 0....   \n",
      "3  [-0.0361063704, -0.163667649, -0.22443682, -0....   \n",
      "4  [-0.190464601, -0.202941269, -0.327967256, -0....   \n",
      "\n",
      "                                     transcript_bert  \\\n",
      "0  [-0.569111705, -0.543804824, -0.141024753, 0.1...   \n",
      "1  [-0.545914114, -0.295345664, -0.202920973, 0.0...   \n",
      "2  [-0.810874343, -0.253732115, -0.495483011, 0.3...   \n",
      "3  [-0.165312082, -0.668395579, -0.659501672, -0....   \n",
      "4  [-0.518009365, -0.229858547, -0.247252136, 0.2...   \n",
      "\n",
      "                                         resume_bert  \\\n",
      "0  [-1.01753056, -0.191111356, -0.31804511, -0.28...   \n",
      "1  [-0.490350217, 0.18670845, -0.368963569, 0.060...   \n",
      "2  [-1.03070152, 0.370450199, -0.449870557, -0.04...   \n",
      "3  [-0.4157016, -0.29293224, -0.16800818, -0.1828...   \n",
      "4  [-0.803099036, 0.252510846, -0.238545656, 0.10...   \n",
      "\n",
      "                            job_description_word2vec  \\\n",
      "0  [0.5068023, -0.12546456, 0.7389149, -0.6894486...   \n",
      "1  [1.0229793, 0.05989623, 1.4022692, -0.75934297...   \n",
      "2  [0.9270255, 0.0133434, 1.7455078, -0.71988434,...   \n",
      "3  [0.21010208, 0.41417825, 1.2362776, -1.1720171...   \n",
      "4  [0.85659266, -0.10477389, 1.6043124, -0.852205...   \n",
      "\n",
      "                                 transcript_word2vec  \\\n",
      "0  [0.01075406, 0.7207075, 0.16426691, -0.1420609...   \n",
      "1  [0.09987153, 0.7372747, 0.06237955, -0.1804453...   \n",
      "2  [0.1902774, 0.549441, 0.4270309, 0.02757834, 0...   \n",
      "3  [0.16780676, 0.743048, 0.17947781, -0.1912264,...   \n",
      "4  [0.3677691, 0.41351652, 0.10227631, -0.1352370...   \n",
      "\n",
      "                                     resume_word2vec  \n",
      "0  [-0.6371148, 0.80693257, 0.11435751, -0.483057...  \n",
      "1  [0.12514763, 0.6327575, 0.63175255, -0.3424733...  \n",
      "2  [0.20390636, 0.4078928, 0.9611415, -0.03728159...  \n",
      "3  [-0.4162535, 1.015284, 0.7123761, -0.4110522, ...  \n",
      "4  [0.03200936, 0.61920214, 0.6882263, -0.1802743...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel('B:\\OneDrive - Amity University\\Desktop\\Tower Research\\processed_dataset_with_embeddings.xlsx')\n",
    "\n",
    "# Example: Assuming the embeddings are in columns 6, 7, and 8 (adjust according to your actual column names)\n",
    "embedding_columns = ['job_description_bert', 'transcript_bert', 'resume_bert','job_description_word2vec','transcript_word2vec','resume_word2vec']  # replace with your actual column names\n",
    "\n",
    "# Function to convert the string of numbers into a list of floats\n",
    "def convert_to_float(embedding_str):\n",
    "    embedding_list = embedding_str.strip('[]').split()  # remove the brackets and split the numbers\n",
    "    return [float(num) for num in embedding_list]  # convert each number to float\n",
    "\n",
    "# Apply the function to the relevant columns\n",
    "for col in embedding_columns:\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "\n",
    "# Optionally, check the first few rows of the DataFrame to confirm\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV if needed\n",
    "df.to_csv('modified_embeddings_bert_word2vec.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pre proccesing the glove embeddings for further work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "               source_file source_sheet  \\\n",
      "0  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "1  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "2  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "3  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "4  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "\n",
      "                           job_description_embedding  \\\n",
      "0  [-0.06838082, 0.16142741, 0.08740371, -0.03329...   \n",
      "1  [-0.07749827, 0.18295107, 0.09905753, -0.03773...   \n",
      "2  [-0.07265462, 0.17151662, 0.09286644, -0.03537...   \n",
      "3  [-0.07265462, 0.17151662, 0.09286644, -0.03537...   \n",
      "4  [-0.06838082, 0.16142741, 0.08740371, -0.03329...   \n",
      "\n",
      "                                transcript_embedding  \\\n",
      "0  [-0.0774327271, 0.147802513, 0.225390829, -0.0...   \n",
      "1  [-0.0943983156, 0.127096921, 0.184848412, -0.0...   \n",
      "2  [-0.09132301, 0.15501445, 0.18994288, -0.10643...   \n",
      "3  [-0.0975857525, 0.163222039, 0.178800796, -0.1...   \n",
      "4  [-0.08310437, 0.1506663, 0.1978036, -0.0860924...   \n",
      "\n",
      "                                    resume_embedding  \n",
      "0  [-0.0925216397, 0.110513205, 0.116988403, -0.0...  \n",
      "1  [-0.134120598, 0.116481611, 0.0549653474, 0.00...  \n",
      "2  [-0.121016204, 0.164515164, 0.0728409408, 0.02...  \n",
      "3  [-0.11706987, 0.14475598, 0.05192951, -0.01342...  \n",
      "4  [-0.119422166, 0.140347939, 0.0922491961, 0.01...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel('B:\\OneDrive - Amity University\\Desktop\\Tower Research\\processed_dataset_with_embeddings_glove_2.xlsx')\n",
    "\n",
    "# Example: Assuming the embeddings are in columns 6, 7, and 8 (adjust according to your actual column names)\n",
    "embedding_columns = ['job_description_embedding', 'transcript_embedding','resume_embedding']  # replace with your actual column names\n",
    "\n",
    "# Function to convert the string of numbers into a list of floats\n",
    "def convert_to_float(embedding_str):\n",
    "    embedding_list = embedding_str.strip('[]').split()  # remove the brackets and split the numbers\n",
    "    return [float(num) for num in embedding_list]  # convert each number to float\n",
    "\n",
    "# Apply the function to the relevant columns\n",
    "for col in embedding_columns:\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "\n",
    "# Optionally, check the first few rows of the DataFrame to confirm\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV if needed\n",
    "df.to_csv('modified_embeddings_glove.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training the 3 types of model embeddings in XG BOOST and ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ANN model...\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "82/82 [==============================] - 3s 14ms/step - loss: 0.4026 - accuracy: 0.8123 - val_loss: 0.3041 - val_accuracy: 0.8231\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2844 - accuracy: 0.8415 - val_loss: 0.2967 - val_accuracy: 0.8123\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 0.2510 - accuracy: 0.8573 - val_loss: 0.2845 - val_accuracy: 0.8292\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 0.2372 - accuracy: 0.8669 - val_loss: 0.3009 - val_accuracy: 0.8308\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2274 - accuracy: 0.8665 - val_loss: 0.3273 - val_accuracy: 0.8092\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2118 - accuracy: 0.8819 - val_loss: 0.2989 - val_accuracy: 0.8185\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2081 - accuracy: 0.8835 - val_loss: 0.3114 - val_accuracy: 0.8123\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 1s 8ms/step - loss: 0.1935 - accuracy: 0.8919 - val_loss: 0.3420 - val_accuracy: 0.7954\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.1799 - accuracy: 0.9042 - val_loss: 0.3188 - val_accuracy: 0.8138\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.1629 - accuracy: 0.9188 - val_loss: 0.3502 - val_accuracy: 0.8123\n",
      "21/21 [==============================] - 0s 3ms/step\n",
      "ANN Model Accuracy: 0.8123\n",
      "\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [23:32:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Accuracy: 0.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models saved successfully as 'ann_model.h5', 'xgb_model.json', and 'ann_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "def process_embedding_string(embedding_str):\n",
    "    \"\"\"Convert string representation of embeddings to numpy array\"\"\"\n",
    "    try:\n",
    "        # Convert string representation of list to actual list\n",
    "        return np.array(ast.literal_eval(embedding_str))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Load BERT, Word2Vec, and GloVe embeddings CSV files\n",
    "bert_word2vec_df = pd.read_csv(r'B:\\OneDrive - Amity University\\Desktop\\Tower Research\\Modify\\modified_embeddings_bert_word2vec.csv')\n",
    "glove_df = pd.read_csv(r'B:\\OneDrive - Amity University\\Desktop\\Tower Research\\Modify\\modified_embeddings_glove.csv')\n",
    "\n",
    "# Convert the 'decision' column into binary labels\n",
    "decision_mapping = {'select': 1, 'Select': 1, 'selected': 1, 'rejected': 0, 'Reject': 0, 'reject': 0}\n",
    "bert_word2vec_df['decision1'] = bert_word2vec_df['decision'].map(decision_mapping)\n",
    "glove_df['decision1'] = glove_df['decision'].map(decision_mapping)\n",
    "\n",
    "# Process embedding columns in both dataframes\n",
    "embedding_columns_bert = bert_word2vec_df.columns.difference(['decision', 'decision1', 'id', 'name', 'role', 'transcript', 'resume', 'reason_for_decision', 'job_description', 'source_file', 'source_sheet'])\n",
    "embedding_columns_glove = glove_df.columns.difference(['decision', 'decision1', 'id', 'name', 'role', 'transcript', 'resume', 'reason_for_decision', 'job_description', 'source_file', 'source_sheet'])\n",
    "\n",
    "# Convert embedding strings to numpy arrays\n",
    "for col in embedding_columns_bert:\n",
    "    bert_word2vec_df[col] = bert_word2vec_df[col].apply(process_embedding_string)\n",
    "for col in embedding_columns_glove:\n",
    "    glove_df[col] = glove_df[col].apply(process_embedding_string)\n",
    "\n",
    "# Create feature matrix by concatenating all embedding vectors\n",
    "def create_feature_matrix(df, embedding_columns):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        combined_embedding = np.concatenate([row[col] for col in embedding_columns if row[col] is not None])\n",
    "        features.append(combined_embedding)\n",
    "    return np.vstack(features)\n",
    "\n",
    "# Create feature matrices\n",
    "X_bert_word2vec = create_feature_matrix(bert_word2vec_df, embedding_columns_bert)\n",
    "X_glove = create_feature_matrix(glove_df, embedding_columns_glove)\n",
    "\n",
    "# Combine all features\n",
    "X = np.hstack([X_bert_word2vec, X_glove])\n",
    "y = bert_word2vec_df['decision1'].values\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train ANN Model\n",
    "def train_ann(X_train, y_train, X_test, y_test):\n",
    "    ann_model = Sequential()\n",
    "    ann_model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "    ann_model.add(Dropout(0.2))\n",
    "    ann_model.add(Dense(64, activation='relu'))\n",
    "    ann_model.add(Dropout(0.2))\n",
    "    ann_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    ann_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    ann_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    y_pred_ann = (ann_model.predict(X_test) > 0.5).astype(int)\n",
    "    ann_accuracy = accuracy_score(y_test, y_pred_ann)\n",
    "    print(f'ANN Model Accuracy: {ann_accuracy:.4f}')\n",
    "    return ann_model\n",
    "\n",
    "# Train XGBoost Model\n",
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f'XGBoost Model Accuracy: {xgb_accuracy:.4f}')\n",
    "    return xgb_model\n",
    "\n",
    "# Train models\n",
    "print(\"Training ANN model...\")\n",
    "ann_model = train_ann(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_model = train_xgboost(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Save models\n",
    "ann_model.save('ann_model.h5')\n",
    "xgb_model.save_model('xgb_model.json')\n",
    "joblib.dump(ann_model, 'ann_model.pkl')\n",
    "\n",
    "print(\"\\nModels saved successfully as 'ann_model.h5', 'xgb_model.json', and 'ann_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre Proccessing the File for further work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "   Unnamed: 8  Unnamed: 9  ... transcript_job_keyword_overlap role_popularity  \\\n",
      "0         NaN         NaN  ...                             10             514   \n",
      "1         NaN         NaN  ...                             10             551   \n",
      "2         NaN         NaN  ...                              7             509   \n",
      "3         NaN         NaN  ...                              8             291   \n",
      "4         NaN         NaN  ...                              9             592   \n",
      "\n",
      "  decision_reason_encoded Unnamed: 45 resume_job_similarity  \\\n",
      "0                      54         NaN                   1.0   \n",
      "1                      54         NaN                   1.0   \n",
      "2                      54         NaN                   1.0   \n",
      "3                      54         NaN                   1.0   \n",
      "4                       5         NaN                   1.0   \n",
      "\n",
      "  transcript_job_similarity  transcript_resume_similarity  \\\n",
      "0                       1.0                           1.0   \n",
      "1                       1.0                           1.0   \n",
      "2                       1.0                           1.0   \n",
      "3                       1.0                           1.0   \n",
      "4                       1.0                           1.0   \n",
      "\n",
      "   job_description_embedding_glove  transcript_embedding_glove  \\\n",
      "0                               []                          []   \n",
      "1                               []                          []   \n",
      "2                               []                          []   \n",
      "3                               []                          []   \n",
      "4                               []                          []   \n",
      "\n",
      "   resume_embedding_glove  \n",
      "0                      []  \n",
      "1                      []  \n",
      "2                      []  \n",
      "3                      []  \n",
      "4                      []  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(r'B:\\OneDrive - Amity University\\Desktop\\Assignment-5\\glove_word2vec_embss_bert_embedss_EDA_dataset.xlsx')\n",
    "\n",
    "\n",
    "embedding_columns = ['job_description_bert', 'transcript_bert', 'resume_bert', \n",
    "                     'job_description_word2vec', 'transcript_word2vec', 'resume_word2vec', \n",
    "                     'job_description_embedding_glove', 'transcript_embedding_glove', 'resume_embedding_glove']\n",
    "\n",
    "# Function to convert the string of numbers into a list of floats\n",
    "def convert_to_float(embedding_str):\n",
    "    if pd.isnull(embedding_str) or embedding_str == '':\n",
    "        return []  # or np.nan depending on your preferred handling\n",
    "    try:\n",
    "        embedding_list = embedding_str.strip('[]').split()  # remove the brackets and split the numbers\n",
    "        return [float(num) for num in embedding_list]  # convert each number to float\n",
    "    except ValueError:\n",
    "        return []  # or np.nan if conversion fails\n",
    "\n",
    "# Apply the function to the relevant columns\n",
    "for col in embedding_columns:\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "\n",
    "# Optionally, check the first few rows of the DataFrame to confirm\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new Excel file\n",
    "df.to_excel(r'pre_glove_word2vec_embss_bert_embedss_EDA_dataset.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training XG BOOST and ANN on EDA FEATURES with BERT,GLOVE AND WORD2VEC EMBEDSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1108: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1113: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1133: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ANN model...\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "82/82 [==============================] - 3s 7ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6930 - val_accuracy: 0.5308\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6927 - val_accuracy: 0.5308\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6927 - val_accuracy: 0.5308\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6927 - val_accuracy: 0.5308\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6926 - val_accuracy: 0.5308\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6926 - val_accuracy: 0.5308\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6926 - val_accuracy: 0.5308\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6926 - val_accuracy: 0.5308\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6925 - val_accuracy: 0.5308\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6925 - val_accuracy: 0.5308\n",
      "21/21 [==============================] - 0s 2ms/step\n",
      "ANN Model Accuracy: 0.5308\n",
      "\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [22:26:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Accuracy: 0.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ann_model_final.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "# Path to your dataset\n",
    "final_path_file = r'B:\\OneDrive - Amity University\\Desktop\\Assignment-5\\pre_glove_word2vec_embss_bert_embedss_EDA_dataset.xlsx'\n",
    "combine_df_final = pd.read_excel(final_path_file)\n",
    "\n",
    "# Mapping for decision encoding\n",
    "decision_mapping_final = {'select': 1, 'Select': 1, 'selected': 1, 'rejected': 0, 'Reject': 0, 'reject': 0}\n",
    "combine_df_final['decision_encoded'] = combine_df_final['decision'].map(decision_mapping_final)\n",
    "\n",
    "# Embedding and other feature columns\n",
    "features = [\n",
    "    'job_description_bert', 'transcript_bert', 'resume_bert', 'job_description_word2vec', 'transcript_word2vec', \n",
    "    'resume_word2vec', 'job_description_embedding_glove', 'transcript_embedding_glove', 'resume_embedding_glove',\n",
    "    'resume_word_count', 'resume_char_count', 'resume_avg_word_length', 'resume_sentence_count',\n",
    "    'resume_uppercase_ratio', 'resume_technical_keyword_count', 'resume_positive_keyword_count',\n",
    "    'resume_negative_keyword_count', 'resume_unique_word_ratio', 'transcript_word_count',\n",
    "    'transcript_char_count', 'transcript_avg_word_length', 'transcript_sentence_count',\n",
    "    'transcript_uppercase_ratio', 'transcript_positive_keyword_count', 'transcript_negative_keyword_count',\n",
    "    'transcript_unique_word_ratio', 'job_role_in_resume', 'resume_job_keyword_overlap',\n",
    "    'transcript_job_keyword_overlap', 'role_popularity', 'decision_reason_encoded',\n",
    "    'resume_job_similarity', 'transcript_job_similarity', 'transcript_resume_similarity'\n",
    "]\n",
    "target = 'decision_encoded'  \n",
    "\n",
    "# Convert string representations of lists into actual lists\n",
    "def convert_to_list(value):\n",
    "    try:\n",
    "        # Safely evaluate the string into a list\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If conversion fails, return an empty list (or you can handle this differently)\n",
    "        return []\n",
    "\n",
    "# Apply the conversion function to embedding columns\n",
    "embedding_columns = [\n",
    "    'job_description_bert', 'transcript_bert', 'resume_bert',\n",
    "    'job_description_word2vec', 'transcript_word2vec', 'resume_word2vec',\n",
    "    'job_description_embedding_glove', 'transcript_embedding_glove', 'resume_embedding_glove'\n",
    "]\n",
    "\n",
    "for col in embedding_columns:\n",
    "    combine_df_final[col] = combine_df_final[col].apply(convert_to_list)\n",
    "\n",
    "# Flatten embedding columns to a single numeric value (e.g., by taking the mean of the list)\n",
    "def flatten_embedding_column(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(lambda x: np.mean(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Apply the flatten function to all embedding columns\n",
    "for col in embedding_columns:\n",
    "    flatten_embedding_column(combine_df_final, col)\n",
    "\n",
    "# Now, prepare the features and target\n",
    "X = combine_df_final[features]\n",
    "y = combine_df_final[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train ANN Model\n",
    "def train_ann(X_train, y_train, X_test, y_test):\n",
    "    ann_model = Sequential()\n",
    "    ann_model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "    ann_model.add(Dropout(0.2))\n",
    "    ann_model.add(Dense(64, activation='relu'))\n",
    "    ann_model.add(Dropout(0.2))\n",
    "    ann_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    ann_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    ann_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    y_pred_ann = (ann_model.predict(X_test) > 0.5).astype(int)\n",
    "    ann_accuracy = accuracy_score(y_test, y_pred_ann)\n",
    "    print(f'ANN Model Accuracy: {ann_accuracy:.4f}')\n",
    "    return ann_model\n",
    "\n",
    "# Train XGBoost Model\n",
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f'XGBoost Model Accuracy: {xgb_accuracy:.4f}')\n",
    "    return xgb_model\n",
    "\n",
    "# Train models\n",
    "print(\"Training ANN model...\")\n",
    "ann_model = train_ann(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_model = train_xgboost(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Save models\n",
    "ann_model.save('ann_model_final.h5')  # Keras model in .h5 format\n",
    "xgb_model.save_model('xgb_model_final.json')  # XGBoost model in .json format\n",
    "\n",
    "# Save the scaler (optional)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Saving the ANN model with joblib for later loading (optional)\n",
    "joblib.dump(ann_model, 'ann_model_final.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
